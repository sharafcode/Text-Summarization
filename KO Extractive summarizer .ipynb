{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FrequencySummarizer:\n",
    "    def __init__(self, min_cut=0.01, max_cut=0.9):\n",
    "        \"\"\"\n",
    "        Initilize the text summarizer.\n",
    "        Words that have a frequency term lower than min_cut \n",
    "        or higher than max_cut will be ignored.\n",
    "        \"\"\"\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut \n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "    def _compute_frequencies(self, word_sent):\n",
    "        \"\"\" \n",
    "        Compute the frequency of each of word.\n",
    "        Input: \n",
    "        word_sent, a list of sentences already tokenized.\n",
    "        Output: \n",
    "        freq, a dictionary where freq[w] is the frequency of w.\n",
    "        \"\"\"\n",
    "        freq = defaultdict(int)\n",
    "        for s in word_sent:\n",
    "            for word in s:\n",
    "                if word not in self._stopwords:\n",
    "                    freq[word] += 1\n",
    "        # frequencies normalization and filtering\n",
    "        m = float(max(freq.values()))\n",
    "        for w in freq.keys():\n",
    "            freq[w] = freq[w]/m\n",
    "            if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "                del freq[w]\n",
    "        return freq\n",
    "    \n",
    "    def Text_length (self,text):\n",
    "        sents= sent_tokenize(text)\n",
    "        return len(sents)\n",
    "    \n",
    "    def summarize(self, text, n):\n",
    "        \"\"\"\n",
    "        Return a list of n sentences\n",
    "        which represent the summary of text.\n",
    "        \"\"\"\n",
    "        sents = sent_tokenize(text)\n",
    "        assert n <= len(sents)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        ranking = defaultdict(int)\n",
    "        for i,sent in enumerate(word_sent):\n",
    "            for w in sent:\n",
    "                if w in self._freq:\n",
    "                    ranking[i] += self._freq[w]\n",
    "        sents_idx = self._rank(ranking, n)\n",
    "        return [sents[j] for j in sents_idx]\n",
    "    \n",
    "    def _rank(self, ranking, n):\n",
    "        \"\"\" return the first n sentences with highest ranking \"\"\"\n",
    "        return nlargest(n, ranking, key=ranking.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_only_text(url):\n",
    "    \"\"\" \n",
    "    return the title and the text of the article\n",
    "    at the specified url\n",
    "     \"\"\"\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "    req = urllib2.Request(url, headers=hdr)\n",
    "    try:\n",
    "        page = urllib2.urlopen(req).read().decode('utf8')\n",
    "    except urllib2.HTTPError, e:\n",
    "        print e.fp.read()\n",
    "        \n",
    "    soup = BeautifulSoup(page)\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    "    \n",
    "    return soup.title.text, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdel-Rahman\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "Why We Chose Typescript – Upvoted\n",
      "--------------------------------------------------------------\n",
      "Size of text: 49  Sentences\n",
      "Sentence Retrevied from each text relative to the size: 6\n",
      "______________________________________________________________________________________________________________\n",
      "* Typescript also came with a lot of “social proof” and better assurances about its longevity There are several large projects using Typescript (examples include VSCode, Rxjs, Angular, and Typescript itself), so we felt confident that its feature set could support our product goals, and the language would stick around for several years.\n",
      "* One major difference between Typescript and Flow is that Typescript is a language that compiles down to Javascript, whereas Flow is a set of annotations you can add to existing Javascript that can then be checked for correctness by a tool.\n",
      "* We picked Typescript because we are confident we could onboard devs quickly (the number of frontend engineers has tripled in the last year), the language could support our product goals of redesigning the entire site, it would stick around for a while, and it would integrate well with our codebase.\n",
      "* Each language comes with its pros and cons, so to aid in picking one, we needed to establish a few requirements: After considering these requirements, our two best options seemed to be either Typescript or Javascript + Flow.\n",
      "* Using a typed language in our frontend has already paid dividends: our code has fewer type-related bugs, we are more confident making large refactors, and our inline documentation is focused around concepts instead of object shapes and function parameters.\n",
      "* That means in Flow, this will throw an error: But in Typescript, this is okay: There are more examples you can find online, but the general consensus is that Flow does a better job at type-checking compared to Typescript.\n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "How A Box Of Cereal And Being Like A Cockroach Helped Airbnb Become A Billion-Dollar Business - Business Insider\n",
      "--------------------------------------------------------------\n",
      "Size of text: 22  Sentences\n",
      "Sentence Retrevied from each text relative to the size: 2\n",
      "______________________________________________________________________________________________________________\n",
      "* Apple is lagging the market as iPhone 8 woes mount \n",
      "More \"The Bottom Line\" »  Here's how much you need to save for college every year depending on when you start \n",
      "More \"Year by Year\" »  Get the best of Business Insider delivered to your inbox every day.\n",
      "* Chesky says\n",
      "  the interview didn't go well, but in a last-ditch effort, his\n",
      "  co-founder whipped out a box of Obama O's and handed it to\n",
      "  Graham.\n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Diligence at Social Capital Part 2: Accounting for Revenue Growth\n",
      "--------------------------------------------------------------\n",
      "Size of text: 55  Sentences\n",
      "Sentence Retrevied from each text relative to the size: 6\n",
      "______________________________________________________________________________________________________________\n",
      "* The explicit identity is: MRR(t) = new(t) + retained(t) + resurrected(t) + expansion(t) MRR(t - 1 month) = retained(t) + churned(t) + contraction(t) Note that if a customer spends $10 in the first month and $12 in the second month we are counting $10 as retained spending and $2 as expansion spending and similarly for contraction.\n",
      "* The above can be rearranged as follows: MRR(t) - MRR(t - 1 month) = new(t) + resurrected(t) + expansion(t) - churned(t) - contraction(t) Which are the five components illustrated here: Once again, we compute the dollar based “quick ratio” which comes out to between 1 and 1.5 depending on the month and the dollar based retention rate which is here ~40%.\n",
      "* In the previous post of this series we looked at growth accounting for users by showing monthly active user (MAU) growth accounting and how it could uncover different underlying dynamics for a growing user base.\n",
      "* However, it’s largely eaten up by contraction revenue leading to a situation where they have to generate expansion MRR just to outrun contraction MRR to generate net growth.\n",
      "* This will be particularly useful for thinking about recurring revenue as would be exhibited in an enterprise software-as-a-service (SaaS) business or in a consumer subscription business.\n",
      "* When counting dollars, a single customer may be retained or churned but they may also be retained as a customer by spending more or less in the second period relative to the first.\n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "The Keras Blog\n",
      "--------------------------------------------------------------\n",
      "Size of text: 134  Sentences\n",
      "Sentence Retrevied from each text relative to the size: 16\n",
      "______________________________________________________________________________________________________________\n",
      "* Now, \n",
      "imagine neural networks that would be \"augmented\" in a similar way with programming primitives such as for loops—but not just a single \n",
      "hard-coded for loop with a hard-coded geometric memory, rather, a large set of programming primitives that the model would be free to manipulate to expand its \n",
      "processing function, such as if branches, while statements, variable creation, disk storage for long-term memory, \n",
      "sorting operators, advanced datastructures like lists, graphs, and hashtables, and many more.\n",
      "* One can imagine a future where models that would be globally non-differentiable (but would feature differentiable parts) would be \n",
      "trained—grown—using an efficient search process that would not leverage gradients, while the differentiable parts would be trained even faster \n",
      "by taking advantage of gradients using some more efficient version of backpropagation.\n",
      "* In particular, I would expect the \n",
      "emergence of a crossover subfield in-between deep learning and program synthesis, where we would not quite be generating programs in a \n",
      "general-purpose language, but rather, where we would be generating neural networks (geometric data processing flows) augmented with a \n",
      "rich set of algorithmic primitives, such as for loops—and many others.\n",
      "* Because training a new model from scratch \n",
      "every time we try a slightly different architecture is tremendously inefficient, a truly powerful AutoML system would manage to evolve \n",
      "architectures at the same time as the features of the model are being tuned via backprop on the training data, thus eliminating all computational redundancy.\n",
      "* When the system would find itself developing similar \n",
      "program subroutines for several different tasks, if would come up with an \"abstract\", reusable version of the subroutine and would store it \n",
      "in the global library.\n",
      "* In a word, we will move away from having on one hand \"hard-coded algorithmic intelligence\" (handcrafted software) and on the other hand \n",
      "\"learned geometric intelligence\" (deep learning).\n",
      "* If machine learning models become more like programs, then they will mostly no longer be differentiable—certainly, these programs will \n",
      "still leverage continuous geometric layers as subroutines, which will be differentiable, but the model as a whole would not be.\n",
      "* If models get more complex and are built on top of richer algorithmic primitives, then this increased complexity will require higher \n",
      "reuse between tasks, rather than training a new model from scratch every time we have a new task or a new dataset.\n",
      "* Such a process would implement the capability for abstraction, a necessary component for achieving \"extreme \n",
      "generalization\": a subroutine that is found to be useful across different tasks and domains can be said to \"abstract\" some aspect of problem-solving.\n",
      "* As you can see, is it highly reminiscent of machine learning: given \n",
      "\"training data\" provided as input-output pairs, we find a \"program\" that matches inputs to outputs and can generalize to new inputs.\n",
      "* In the future, I would expect a generalized version of this \n",
      "to be commonplace: we would not only leverage previously learned features (submodel weights), but also model architectures and training \n",
      "procedures.\n",
      "* But our models \n",
      "will certainly become increasingly more ambitious than mere differentiable parametric functions, \n",
      "and thus their automatic development (the \"learning\" in \"machine learning\") will require more than backpropagation.\n",
      "* The space of programs that such a network could represent would be far broader than what can be represented with current deep learning models, \n",
      "and some of these programs could achieve superior generalization power.\n",
      "* Currently, most of the job of a deep learning engineer consists in munging data with Python scripts, then lengthily tuning the architecture \n",
      "and hyperparameters of a deep network to get a working model—or even, to get to a state-of-the-art model, if the engineer is so \n",
      "ambitious.\n",
      "* As we noted in our previous post, a necessary transformational development that we can expect in the field of machine learning is a move away \n",
      "from models that perform purely pattern recognition and can only achieve local generalization, towards models capable of abstraction and \n",
      "reasoning, that can achieve extreme generalization.\n",
      "* Training an image classification model jointly with an image segmentation model, sharing the same convolutional base, results in a \n",
      "model that is better at both tasks.\n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "The Heartbeat of Product by Nate Walkingshaw - Mind the Product\n",
      "--------------------------------------------------------------\n",
      "Size of text: 28  Sentences\n",
      "Sentence Retrevied from each text relative to the size: 3\n",
      "______________________________________________________________________________________________________________\n",
      "* BY MARTIN ERIKSSON ON JULY 21, 2017 Nate Walkingshaw is the Chief Experience Officer for Pluralsight, where he is responsible for Product, User Experience, Engineering, and Content, and the co-author of Product Leadership: How Top Product Managers Launch Awesome Products and Build Successful Teams.\n",
      "* But he started his career as an EMT (Emergency Medical Technician) at $7.14 an hour, and in this heartfelt talk from Mind the Product San Francisco 2017 he shares how his emergency medical experience and training set him up for success in building products and companies – and what we can learn from it too.\n",
      "* Nate lays out his approach to understanding the heartbeat of a product team, and how to make sure it’s a healthy heartbeat – it’s a masterclass in how to build amazing products and the teams that deliver them.\n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Breaking (Bad) Mantras – Point Nine Land – Medium\n",
      "--------------------------------------------------------------\n",
      "Size of text: 62  Sentences\n",
      "Sentence Retrevied from each text relative to the size: 7\n",
      "______________________________________________________________________________________________________________\n",
      "* We can invest earlier, in unproven teams, in new geographies, in less sexy or more competitive markets, in unproven technologies, in new ways of financing businesses… And if you’re lucky enough, you can find 2 designers working as Co-CEOs, building a disruptive product through better design, in Barcelona: Welcome to Typeform!\n",
      "* Look at Slack, it took them 2.5 years to reach 100m in revenue — quite an alive company for a dead trend… But as you might remember from the intro, VC is a lot about trial and error.\n",
      "* In plain words, you must be right when everyone thinks you’re dumb — you’re wrong so often in VC … that you get used to that feeling!\n",
      "* I’ve heard quite often founders complaining that VCs don’t take enough risks.\n",
      "* Once in a while, founders break more than one of those mantras… There are plenty like the previous: Those previous companies became extraordinarily successful because they broke mantras.\n",
      "* But not only that, quite often, when a company does well, there are external signs like fundraising, acquisition talks, key hires, etc.\n",
      "* But equally important, they changed how founders and investors perceived the world: There was a shift in an important aspect of how the world was understood to work “until them” and “after them”.\n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Using Machine Learning to Predict Value of Homes On Airbnb\n",
      "--------------------------------------------------------------\n",
      "Size of text: 70  Sentences\n",
      "Sentence Retrevied from each text relative to the size: 8\n",
      "______________________________________________________________________________________________________________\n",
      "* If a desired feature is not available, a user can create her own feature with a feature configuration file like the following: When multiple features are required for the construction of a training set, Zipline will automatically perform intelligent key joins and backfill the training dataset behind the scenes.\n",
      "* The remainder of this post is organized into four topics, along with the tools we used to tackle each task: One of the first steps of any supervised machine learning project is to define relevant features that are correlated with the chosen outcome variable, a process called feature engineering.\n",
      "* To make it more concrete, below is a code snippet from our LTV model pipeline: At a high level, we use pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric.\n",
      "* As in the example training dataset above, we often need to perform additional data processing before we can fit a model: In this step, we don’t quite know what is the best set of features to use, so writing code that allows us to rapidly iterate is essential.\n",
      "* While one can use past data to calculate the historical value of existing listings, we took one step further to predict LTV of new listings using machine learning.\n",
      "* For example, our ML Infra team built a general feature repository that allows users to leverage high quality, vetted, reusable features in their models.\n",
      "* Luckily, At Airbnb we have machine learning tools that abstract away the engineering work behind productionizing ML models.\n",
      "* Recently, advances in Airbnb’s machine learning infrastructure have lowered the cost significantly to deploy new machine learning models to production.\n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "How to identify Good Design in 6 steps – Muzli -Design Inspiration\n",
      "--------------------------------------------------------------\n",
      "Size of text: 106  Sentences\n",
      "Sentence Retrevied from each text relative to the size: 13\n",
      "______________________________________________________________________________________________________________\n",
      "* Now, this \"tiny\" level of subjectivity doesn’t mean that you can’t tell good design from bad, it just means that you might find a good design “ugly” or in the other side of the spectrum, you might find something that looks beautiful that actually is a pretty bad design.\n",
      "* This should be enough to get you in a good path, nonetheless, in the end of the day, this check point will always be a bit subjective to go through, but since this is just one out of six, it shouldn’t disable you from distinguishing good design from bad.\n",
      "* If the design passed the previous 5 check points, you have already a really good design in front of you, this check point is what takes it from good to extraordinary.\n",
      "* Because it has so many functional problems that I won’t even bother to write them here (just go to amazon and read the reviews if you are interested and/or want to have a laugh, or read this), but in short, it doesn’t do a good job in the only thing that it’s supposed to do, help you to get that fresh juice.\n",
      "* You just need to look beyond the looks, and to help you do that I devised a list of 6 \"check points\" that you can use to filter the good from the bad.\n",
      "* If it doesn’t solve the problem, you don’t need to go any further, it’s definitely not a good design.\n",
      "* If you do that, you’ll start to see the patterns that keep reappearing in good design like well-balanced compositions, beautiful typography, precise alignments, delightful colour combinations and many other things.\n",
      "* Sometimes the client might think that they need one thing but after a few questions you'll realise that they actually need something entirely different.\n",
      "* This to say that it’s possible for something to be \"good looking\" but still be a bad design.\n",
      "* Essentially this is when a design goes beyond a combination of good typography and colours, it’s when there’s a brilliant idea that supports everything and takes it to a whole new level.\n",
      "* It might be a website that needs to be easier to use, a product that needs to appeal to a certain audience or a new business that needs a logo, the problem can be literally anything.\n",
      "* Good designers will rely on their technical skills and base their design on principles (a machine could learn that by the way), but great designers bring more to the equation.\n",
      "* There’s nothing worse than catching the wave too late, this will only make you look bad, like you’re trying to catch up rather than being the one setting the trend.\n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Top 3 UI Techniques That Users Hate Most – UX Planet\n",
      "--------------------------------------------------------------\n",
      "Size of text: 31  Sentences\n",
      "Sentence Retrevied from each text relative to the size: 3\n",
      "______________________________________________________________________________________________________________\n",
      "* However, the technique has a huge negative effect on user — it asks the users to stop doing what they’re doing (what’s important for them) and focuses their attention on completely different activity (what’s important to you).\n",
      "* Infinite scrolling is a technique that allowing users to scroll through a massive chunk of content with no finishing-line in sight.\n",
      "* In general, infinite scroll works well for something like Twitter/Instagram where users consuming an endlessly flowing stream of data without looking for anything in particular.\n",
      "______________________________________________________________________________________________________________\n",
      "\n",
      "--------------------------------------------------------------\n",
      "How Do Venture Capitalists Make Decisions? – The PNR Paper – Medium\n",
      "--------------------------------------------------------------\n",
      "Size of text: 61  Sentences\n",
      "Sentence Retrevied from each text relative to the size: 7\n",
      "______________________________________________________________________________________________________________\n",
      "* LPs need to be wary of this overconfidence in generating above market returns and most likely need to lower VC firms marketed multiples (early stage: 24% net IRR/ 3.8 cash-on-cash, late stage: 21% net IRR/2.8 cash-on-cash).\n",
      "* Surprisingly, VCs report that the most important contributor to value creation is deal selection in front of deal flow and value add, although many VCs will market their proprietary deal flow and value add/founder friendly infrastructure.\n",
      "* The authors of this paper wanted to answer these questions and they did so by surveying almost 900 VCs on multiple areas: deal sourcing, investment selection, valuation tools, deal structure, post-investment value add, exits, internal organization of the firms and relationships with limited partners.\n",
      "* One interesting statistic is that late stage firms usually offer 50% more term sheets per closed deal, indicating greater competition and less proprietary deal flow.\n",
      "* Although California VC firms use more founder friendly terms, overall VCs are not very flexible on terms, particularly on control rights and liquidation rights (pro-rata rights, liquidation preference, anti-dilution protection, valuation, board control, and vesting).\n",
      "* Early stage VCs are more likely to set the valuation using investment amount and target ownership whereas late stage VCs can use more sophisticated methods to get to the company valuation.\n",
      "* Unsurprisingly, given the “Series A Crunch” (see graph below from Mattermark), connecting to investors is more important for early stage than late stage investors.\n",
      "______________________________________________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Just a naive approach to extract some summaries from Knowledge Officer articles from the direct link\n",
    "\n",
    "# Extract KO Articles\n",
    "with open('URLs.txt') as f:\n",
    "    lines = f.readlines()\n",
    "article_url = [line.rstrip('\\n') for line in open('URLs.txt')]\n",
    "\n",
    "fs = FrequencySummarizer()\n",
    "for article in article_url:\n",
    "    title, text = get_only_text(article)\n",
    "    print '--------------------------------------------------------------'\n",
    "    print title\n",
    "    print '--------------------------------------------------------------'\n",
    "    print \"Size of text: \" + str(fs.Text_length(text)) + \"  Sentences\"\n",
    "    length= fs.Text_length(text)\n",
    "    sents_retrieved = int((1./8.)* length)\n",
    "    if sents_retrieved< 1.:\n",
    "        sents_retrieved=2\n",
    "    print \"Sentence Retrevied from each text relative to the size: \"+ str(sents_retrieved) \n",
    "    print \"______________________________________________________________________________________________________________\"\n",
    "    for s in fs.summarize(text, sents_retrieved):   #Here we can control the number of the sentences snippets. \n",
    "        print '*',s\n",
    "    print \"______________________________________________________________________________________________________________\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
